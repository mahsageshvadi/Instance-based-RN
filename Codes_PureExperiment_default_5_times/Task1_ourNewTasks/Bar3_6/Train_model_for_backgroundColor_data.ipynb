{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mahsa.geshvadi001/miniconda3/envs/ZHIHAO/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Resetting the folder..... /home/mahsa.geshvadi001/Instance-based-RN/Codes_PureExperiment_default_5_times/Task1_ourNewTasks/Bar3_6/results/IRNm/\n",
      "Generating TRAIN Dataset: 60000 ----------------\n",
      "   id 0 (obj_num = 3)\n",
      "   id 5000 (obj_num = 4)\n",
      "   id 10000 (obj_num = 4)\n",
      "   id 15000 (obj_num = 6)\n",
      "   id 20000 (obj_num = 3)\n",
      "   id 25000 (obj_num = 3)\n",
      "   id 30000 (obj_num = 5)\n",
      "   id 35000 (obj_num = 6)\n",
      "   id 40000 (obj_num = 3)\n",
      "   id 45000 (obj_num = 5)\n",
      "   id 50000 (obj_num = 6)\n",
      "   id 55000 (obj_num = 6)\n",
      "x_shape:  (6, 60000, 100, 100, 3)\n",
      "y_shape:  (60000, 6)\n",
      "Generating VAL Dataset: 20000 ----------------\n",
      "   id 0 (obj_num = 5)\n",
      "   id 5000 (obj_num = 5)\n",
      "   id 10000 (obj_num = 5)\n",
      "   id 15000 (obj_num = 5)\n",
      "x_shape:  (6, 20000, 100, 100, 3)\n",
      "y_shape:  (20000, 6)\n",
      "Generating TEST Dataset: 20000 ----------------\n",
      "   id 0 (obj_num = 4)\n",
      "   id 5000 (obj_num = 5)\n",
      "   id 10000 (obj_num = 3)\n",
      "   id 15000 (obj_num = 5)\n",
      "x_shape:  (6, 20000, 100, 100, 3)\n",
      "y_shape:  (20000, 6)\n",
      "after generating\n",
      "----------Build Network----------------\n",
      "WARNING:tensorflow:From /home/mahsa.geshvadi001/miniconda3/envs/ZHIHAO/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Level2: 12 12 64\n",
      "ratio_p_layers 6 (?, 1)\n",
      "output layer:  (?, 6)\n",
      "----------Training-------------------\n",
      "WARNING:tensorflow:From /home/mahsa.geshvadi001/miniconda3/envs/ZHIHAO/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/mahsa.geshvadi001/miniconda3/envs/ZHIHAO/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "iter(0/100) Batch(0/1875) [>........................................] : mse_loss=0.23867404460906982\n",
      "iter(0/100) Batch(15/1875) [>........................................] : mse_loss=0.1550687551498413\n",
      "iter(0/100) Batch(30/1875) [>........................................] : mse_loss=0.13026948273181915\n",
      "iter(0/100) Batch(45/1875) [>........................................] : mse_loss=0.12939003109931946\n",
      "iter(0/100) Batch(60/1875) [=>.......................................] : mse_loss=0.08467831462621689\n",
      "iter(0/100) Batch(75/1875) [=>.......................................] : mse_loss=0.05066812410950661\n",
      "iter(0/100) Batch(90/1875) [=>.......................................] : mse_loss=0.02814139425754547\n",
      "iter(0/100) Batch(105/1875) [==>......................................] : mse_loss=0.016046511009335518\n",
      "iter(0/100) Batch(120/1875) [==>......................................] : mse_loss=0.016544777899980545\n",
      "iter(0/100) Batch(135/1875) [==>......................................] : mse_loss=0.009162867441773415\n",
      "iter(0/100) Batch(150/1875) [===>.....................................] : mse_loss=0.010212702676653862\n",
      "iter(0/100) Batch(165/1875) [===>.....................................] : mse_loss=0.005781719461083412\n",
      "iter(0/100) Batch(180/1875) [===>.....................................] : mse_loss=0.006309866905212402\n",
      "iter(0/100) Batch(195/1875) [====>....................................] : mse_loss=0.007691338658332825\n",
      "iter(0/100) Batch(210/1875) [====>....................................] : mse_loss=0.006210027262568474\n",
      "iter(0/100) Batch(225/1875) [====>....................................] : mse_loss=0.0056601399555802345\n",
      "iter(0/100) Batch(240/1875) [=====>...................................] : mse_loss=0.006281057372689247\n",
      "iter(0/100) Batch(255/1875) [=====>...................................] : mse_loss=0.005231439135968685\n",
      "iter(0/100) Batch(270/1875) [=====>...................................] : mse_loss=0.007724775932729244\n",
      "iter(0/100) Batch(285/1875) [======>..................................] : mse_loss=0.0039003302808851004\n",
      "iter(0/100) Batch(300/1875) [======>..................................] : mse_loss=0.006488636136054993\n",
      "iter(0/100) Batch(315/1875) [======>..................................] : mse_loss=0.004629671573638916\n",
      "iter(0/100) Batch(330/1875) [=======>.................................] : mse_loss=0.0031149694696068764\n",
      "iter(0/100) Batch(345/1875) [=======>.................................] : mse_loss=0.00650493148714304\n",
      "iter(0/100) Batch(360/1875) [=======>.................................] : mse_loss=0.0016018662136048079\n",
      "iter(0/100) Batch(375/1875) [========>................................] : mse_loss=0.0038811841513961554\n",
      "iter(0/100) Batch(390/1875) [========>................................] : mse_loss=0.006123754195868969\n",
      "iter(0/100) Batch(405/1875) [========>................................] : mse_loss=0.005931111052632332\n",
      "iter(0/100) Batch(420/1875) [========>................................] : mse_loss=0.0023018519859761\n",
      "iter(0/100) Batch(435/1875) [=========>...............................] : mse_loss=0.006402258761227131\n",
      "iter(0/100) Batch(450/1875) [=========>...............................] : mse_loss=0.0017078800592571497\n",
      "iter(0/100) Batch(465/1875) [=========>...............................] : mse_loss=0.004453141242265701\n",
      "iter(0/100) Batch(480/1875) [==========>..............................] : mse_loss=0.0024991692043840885\n",
      "iter(0/100) Batch(495/1875) [==========>..............................] : mse_loss=0.006499519571661949\n",
      "iter(0/100) Batch(510/1875) [==========>..............................] : mse_loss=0.005086688790470362\n",
      "iter(0/100) Batch(525/1875) [===========>.............................] : mse_loss=0.00440612155944109\n",
      "iter(0/100) Batch(540/1875) [===========>.............................] : mse_loss=0.0027154837734997272\n",
      "iter(0/100) Batch(555/1875) [===========>.............................] : mse_loss=0.005114613100886345\n",
      "iter(0/100) Batch(570/1875) [============>............................] : mse_loss=0.0023605269379913807\n",
      "iter(0/100) Batch(585/1875) [============>............................] : mse_loss=0.004420437850058079\n",
      "iter(0/100) Batch(600/1875) [============>............................] : mse_loss=0.003983008675277233\n",
      "iter(0/100) Batch(615/1875) [=============>...........................] : mse_loss=0.0018322342075407505\n",
      "iter(0/100) Batch(630/1875) [=============>...........................] : mse_loss=0.002738161012530327\n",
      "iter(0/100) Batch(645/1875) [=============>...........................] : mse_loss=0.0015905185136944056\n",
      "iter(0/100) Batch(660/1875) [==============>..........................] : mse_loss=0.0030852663330733776\n",
      "iter(0/100) Batch(675/1875) [==============>..........................] : mse_loss=0.00474883895367384\n",
      "iter(0/100) Batch(690/1875) [==============>..........................] : mse_loss=0.00199545337818563\n",
      "iter(0/100) Batch(705/1875) [===============>.........................] : mse_loss=0.0018358103698119521\n",
      "iter(0/100) Batch(720/1875) [===============>.........................] : mse_loss=0.0030213382560759783\n",
      "iter(0/100) Batch(735/1875) [===============>.........................] : mse_loss=0.0026782394852489233\n",
      "iter(0/100) Batch(750/1875) [================>........................] : mse_loss=0.00735833216458559\n",
      "iter(0/100) Batch(765/1875) [================>........................] : mse_loss=0.011976001784205437\n",
      "iter(0/100) Batch(780/1875) [================>........................] : mse_loss=0.005218776874244213\n",
      "iter(0/100) Batch(795/1875) [================>........................] : mse_loss=0.003164550056681037\n",
      "iter(0/100) Batch(810/1875) [=================>.......................] : mse_loss=0.001464740838855505\n",
      "iter(0/100) Batch(825/1875) [=================>.......................] : mse_loss=0.0015283710090443492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten,Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from  openpyxl import Workbook\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from Configure import Config, GetProcessBar,ReadLinesInFile, ClearDir, MakeDir, RemoveDir\n",
    "import time, argparse\n",
    "from utils.non_local import non_local_block\n",
    "from DataSet_generator_BackgroundColor import GenerateDatasetVGG,GenerateDatasetIRNm\n",
    "import pickle\n",
    "\n",
    "\n",
    "\"\"\" some important configuration \"\"\"\n",
    "train_num = 60000             # image number.\n",
    "val_num   = 20000\n",
    "test_num  = 20000\n",
    "\n",
    "\n",
    "\n",
    "m_epoch = 100                # epoch\n",
    "m_batchSize = 32            # batch_size\n",
    "m_print_loss_step = 15      # print once after how many iterations.\n",
    "\n",
    "exp_id = 1\n",
    "lr = 0.0001\n",
    "savedir = 'IRNm'\n",
    "gpu = 1\n",
    "\n",
    "\"\"\" processing command line \"\"\"\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--times\", default=5, type=int)\n",
    "#parser.add_argument(\"--gpu\", default='0')                  # gpu id\n",
    "#parser.add_argument(\"--lr\", default=0.0001, type = float)  # learning rate\n",
    "#parser.add_argument(\"--savedir\", default= 'IRNm')           # saving path.\n",
    "#parser.add_argument(\"--backup\", default=False, type=bool)   # whether to save weights after each epoch.\n",
    "                                                           # (If True, it will cost lots of memories)\n",
    "#a = parser.parse_args()\n",
    "\n",
    "m_optimizer = Adam(lr)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = 'gpu'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# create save folder.\n",
    "MakeDir(\"./results/\")\n",
    "\n",
    "\n",
    "# Level1 module is to extract the individual features from one instance.\n",
    "# Level1 has two NON-LOCAL block.\n",
    "def Level1_Module():\n",
    "    #todo change this\n",
    "    input = Input(shape=(config.image_height, config.image_width, 3))\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = non_local_block(x)   # non local block\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = non_local_block(x)   # non local block\n",
    "    return Model(inputs=input, outputs=x)\n",
    "\n",
    "# Level2 module is to compute the ratio of a pair.\n",
    "# Level2 has one NON-LOCAL block.\n",
    "def Level2_Module(w,h,c):\n",
    "    print(\"Level2:\", w,h,c)\n",
    "\n",
    "    inputA = Input(shape=(w, h, c))\n",
    "    inputB = Input(shape=(w, h, c))\n",
    "\n",
    "    combined = keras.layers.concatenate([inputA, inputB])   # concatenate them.\n",
    "    z = Conv2D(64, (3, 3), activation='relu',padding='same')(combined)\n",
    "    z = Conv2D(64, (3, 3), activation='relu', padding='same')(z)\n",
    "    z = non_local_block(z)   # non local block\n",
    "    #\n",
    "    z = Flatten()(z)\n",
    "    z = Dense(256, activation=\"relu\")(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    z = Dense(1, activation=\"linear\")(z)  # output the ratio of this pair.\n",
    "\n",
    "    return Model(inputs=[inputA, inputB], outputs=z)\n",
    "\n",
    "\n",
    "\n",
    "# IRN_m is final network to estimate the ratio vectors from multiple input instances.\n",
    "def Build_IRN_m_Network():\n",
    "    # input layers.\n",
    "    input_layers = []\n",
    "    # the first 'obj_num' inputs are corresponding to the input sub-charts.\n",
    "    for i in range(config.max_obj_num):\n",
    "        #todo change this\n",
    "        input = Input(shape=(config.image_height, config.image_width, 3), name=\"input_{}\".format(i))\n",
    "        input_layers.append(input)\n",
    "\n",
    "    # The last input layer is used for representing R1=(o1/o1)=1.0 which is just a constant.\n",
    "    # Here, I would use an extra input layer which is 1-dim and always equal to 1.0 rather than directly using a contant.\n",
    "    # It makes same effect and can avoid some strange compile errors. (I only use TensorFlow before, not way familiar to Keras.)\n",
    "    R1_one_input = Input(shape=(1,),name=\"input_constant_scalar1\",dtype='float32')   # always equal to 1.0.\n",
    "    input_layers.append(R1_one_input)\n",
    "\n",
    "    # First extract individual features.\n",
    "    individual_features = []\n",
    "    level1 = Level1_Module()  # build a level1 module\n",
    "    for i in range(config.max_obj_num):\n",
    "        x = level1(input_layers[i])\n",
    "        individual_features.append(x)\n",
    "\n",
    "    # Use a Level2 module to predict pairwise ratios.\n",
    "    level2 = Level2_Module(w=int(individual_features[0].shape[1]),\n",
    "                           h=int(individual_features[0].shape[2]),\n",
    "                           c=int(individual_features[0].shape[3]))\n",
    "\n",
    "    ratio_p_layers = [R1_one_input]   # pairwise ratio vector. put in' R1=(o1/o1)=1.0 '.\n",
    "    for i in range(config.max_obj_num-1): # compute the ratio of each neighbor pair.\n",
    "        x = level2(inputs = [individual_features[i], individual_features[i+1]])\n",
    "        ratio_p_layers.append(x)\n",
    "\n",
    "    print(\"ratio_p_layers\", len(ratio_p_layers), ratio_p_layers[-1].shape)\n",
    "\n",
    "    # Compute the ratios relative to the first object by using MULTIPLY() operation.\n",
    "    ratio_layers = [R1_one_input]  # put in R1=1.0.\n",
    "    i = 1\n",
    "    while i<len(ratio_p_layers):\n",
    "        x = keras.layers.Multiply()(ratio_p_layers[:i+1])   # R1*R2*...Ri\n",
    "        i+=1\n",
    "        ratio_layers.append(x)\n",
    "\n",
    "    # divide the maxinum of 'ratio_layers' to get the final results.\n",
    "    max = keras.layers.maximum(ratio_layers)\n",
    "    z = keras.layers.concatenate(ratio_layers)\n",
    "    z = keras.layers.Lambda(lambda x: x[0]/x[1])([z, max])\n",
    "\n",
    "    print(\"output layer: \", z.shape)\n",
    "\n",
    "    return Model(inputs=input_layers, outputs=z)\n",
    "\n",
    "# save the 'predicted results' and 'ground truth' into the file.\n",
    "def SavePredictedResult(dir_results, x, y, flag = 'train'):\n",
    "    dim = y.shape[1]\n",
    "    print(y.shape)\n",
    "    input_test = [x[i] for i in range(config.max_obj_num)]\n",
    "    input_test.append(np.ones(x[0].shape[0]))\n",
    "    predict_Y = model.predict(x=input_test, batch_size=1)\n",
    "    predictFile = open(dir_results + flag + \"_predicted_results.txt\",'w')\n",
    "    for i in range(y.shape[0]):\n",
    "        for t in range(dim):  # save the ground_truth\n",
    "            predictFile.write(str(y[i,t]) + '\\t')\n",
    "        predictFile.write('\\n')\n",
    "        for t in range(dim):  # save the predicted results.\n",
    "            predictFile.write(str(predict_Y[i, t]) + '\\t')\n",
    "        predictFile.write('\\n')\n",
    "    predictFile.close()\n",
    "\n",
    "    MLAE = np.log2(sklearn.metrics.mean_absolute_error( predict_Y * 100, y * 100) + .125)\n",
    "\n",
    "    return MLAE, y, predict_Y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        dir_rootpath = os.path.abspath(\".\") + \"/results/{}/\".format(savedir)    # ./results/network_name/\n",
    "        ClearDir(dir_rootpath)\n",
    "        dir_results = dir_rootpath+  \"{}_{}/\".format( savedir, exp_id )\n",
    "        ClearDir(dir_results)\n",
    "        ClearDir(dir_results + \"backup\")\n",
    "\n",
    "        x_train, y_train = GenerateDatasetIRNm(flag='train', image_num=train_num)\n",
    "        x_val, y_val = GenerateDatasetIRNm(flag='val', image_num=val_num)\n",
    "        x_test, y_test = GenerateDatasetIRNm(flag='test', image_num=test_num)\n",
    "        print(\"after generating\")\n",
    "        x_train -= .5\n",
    "        x_val -= .5\n",
    "        x_test -= .5\n",
    "\n",
    "        # format the inputs as [img1, img2, img3,....., 1.0]\n",
    "        # Note that we add R1=1.0 in the last.\n",
    "        x_train = [x_train[i] for i in range(config.max_obj_num)]\n",
    "        x_val = [x_val[i] for i in range(config.max_obj_num)]\n",
    "        x_test = [x_test[i] for i in range(config.max_obj_num)]\n",
    "        x_train.append(np.ones(train_num))  # Train_num, R1 = 1.0\n",
    "        x_val.append(np.ones(val_num))  # Val_num, R1 = 1.0\n",
    "        x_test.append(np.ones(test_num))  # Test_num, R1 = 1.0\n",
    "\n",
    "        print(\"----------Build Network----------------\")\n",
    "        model = Build_IRN_m_Network()\n",
    "        model.compile(loss='mse', optimizer=m_optimizer)\n",
    "\n",
    "        print(\"----------Training-------------------\")\n",
    "        history_batch = []\n",
    "        history_iter = []\n",
    "        batch_amount = train_num // m_batchSize\n",
    "        rest_size = train_num - (batch_amount * m_batchSize)\n",
    "\n",
    "        # information of the best model on validation set.\n",
    "        best_val_loss = 99999.99999\n",
    "        best_model_name = \"xxxxx\"\n",
    "        best_train_loss = 99999.99999\n",
    "\n",
    "        iter = 0\n",
    "        while iter < m_epoch:\n",
    "\n",
    "            # shuffle the training set !!!!!!!!!!!!!!\n",
    "            index = [i for i in range(train_num)]\n",
    "            np.random.shuffle(index)\n",
    "\n",
    "            for bid in range(batch_amount):\n",
    "\n",
    "                # using the shuffle index...\n",
    "                x_batch = [x_train[i][index[bid * m_batchSize: (bid + 1) * m_batchSize]] for i in\n",
    "                           range(config.max_obj_num + 1)]\n",
    "                y_batch = y_train[index[bid * m_batchSize: (bid + 1) * m_batchSize]]\n",
    "\n",
    "                model.train_on_batch(x=x_batch, y=y_batch)  # training on batch\n",
    "\n",
    "                if bid % m_print_loss_step == 0:\n",
    "                    logs = model.evaluate(x=x_batch, y=y_batch, verbose=0, batch_size=m_batchSize)\n",
    "                    print(\"iter({}/{}) Batch({}/{}) {} : mse_loss={}\".format(iter, m_epoch, bid, batch_amount,\n",
    "                                                                             GetProcessBar(bid, batch_amount), logs))\n",
    "                    history_batch.append([iter, bid, logs])\n",
    "\n",
    "            # training on the rest data.\n",
    "            if rest_size > 0:\n",
    "                model.train_on_batch(\n",
    "                    x=[x_train[i][index[batch_amount * m_batchSize:]] for i in range(config.max_obj_num + 1)],\n",
    "                    y=y_train[index[batch_amount * m_batchSize:]])\n",
    "\n",
    "            # one epoch is done. Do some information collections.\n",
    "            epoch_loss_train = model.evaluate(x=x_train, y=y_train, verbose=0, batch_size=m_batchSize)\n",
    "            epoch_loss_val = model.evaluate(x=x_val, y=y_val, verbose=0, batch_size=m_batchSize)\n",
    "            history_iter.append([iter, epoch_loss_train, epoch_loss_val])\n",
    "            print(\"----- epoch({}/{}) train_loss={}, val_loss={}\".format(iter, m_epoch, epoch_loss_train, epoch_loss_val))\n",
    "\n",
    "            # to avoid stuck in local optimum at the beginning\n",
    "            iter += 1\n",
    "            if iter >= 20 and best_train_loss > 0.05:\n",
    "                history_iter.clear()\n",
    "                history_batch.clear()\n",
    "                best_train_loss = best_val_loss = 999999.\n",
    "\n",
    "                model = Build_IRN_m_Network()\n",
    "                model.compile(loss='mse', optimizer=m_optimizer)\n",
    "                iter = 0\n",
    "                continue\n",
    "\n",
    "            if epoch_loss_val < best_val_loss:  # save the best model on Validation set.\n",
    "                RemoveDir(best_model_name)\n",
    "                best_model_name = dir_results + \"model_irnm_onVal_{}.h5\".format(epoch_loss_val)\n",
    "                model.save_weights(best_model_name)\n",
    "                best_val_loss = epoch_loss_val\n",
    "                best_train_loss = epoch_loss_train\n",
    "\n",
    "        # test on the testing set.\n",
    "        model.load_weights(best_model_name)  # using the best model\n",
    "        test_loss = model.evaluate(x_test, y_test, verbose=0, batch_size=m_batchSize)\n",
    "\n",
    "        # Save the predicted results and return the MLAE.\n",
    "        MLAE_train,_,_ = SavePredictedResult(dir_results, x_train, y_train, 'train')\n",
    "        MLAE_val,_,_ = SavePredictedResult(dir_results, x_val, y_val, 'val')\n",
    "        MLAE_test, _y_test, _y_pred = SavePredictedResult(dir_results, x_test, y_test, 'test')\n",
    "\n",
    "        # save the training information.\n",
    "        wb = Workbook()\n",
    "        ws1 = wb.active  # MSE/MLAE\n",
    "        ws1.title = \"MLAE_MSE\"\n",
    "        ws2 = wb.create_sheet(\"EPOCH loss\")  # iteration loss\n",
    "        ws3 = wb.create_sheet(\"BATCH loss\")  # batch loss\n",
    "\n",
    "        ws2.append([\"Epoch ID\", \"Train MSE Loss\", \"Val MSE Loss\"])\n",
    "        ws3.append([\"Epoch ID\", \"Batch ID\", \"MSE Loss\"])\n",
    "\n",
    "        for i in range(len(history_iter)):\n",
    "            ws2.append(history_iter[i])\n",
    "        for i in range(len(history_batch)):\n",
    "            ws3.append(history_batch[i])\n",
    "\n",
    "        ws1.append([\"Train loss\", best_train_loss])\n",
    "        ws1.append([\"Val loss\", best_val_loss])\n",
    "        ws1.append([\"Test loss\", test_loss])\n",
    "        ws1.append([\"Train MLAE\", MLAE_train])\n",
    "        ws1.append([\"val MLAE\", MLAE_val])\n",
    "        ws1.append([\"Test MLAE\", MLAE_test])\n",
    "\n",
    "        wb.save(dir_results + \"train_info.xlsx\")\n",
    "\n",
    "        print(\"Training MSE:\", best_train_loss)\n",
    "        print(\"Validat. MSE:\", best_val_loss)\n",
    "        print(\"Testing MSE:\", test_loss)\n",
    "        print(\"Training MLAE:\", MLAE_train)\n",
    "        print(\"Validat. MLAE:\", MLAE_val)\n",
    "        print(\"Testing MLAE:\", MLAE_test)\n",
    "\n",
    "        ## save as pickle file\n",
    "\n",
    "        stats = dict()\n",
    "\n",
    "        stats['MSE_train'] = best_train_loss\n",
    "        stats['MSE_val'] = best_val_loss\n",
    "        stats['MSE_test'] = test_loss\n",
    "\n",
    "        stats['MLAE_train'] = MLAE_train\n",
    "        stats['MLAE_val'] = MLAE_val\n",
    "        stats['MLAE_test'] = MLAE_test\n",
    "\n",
    "        stats['loss_train'] = [history_iter[i][1] for i in range(len(history_iter))]\n",
    "        stats['loss_val'] =   [history_iter[i][2] for i in range(len(history_iter))]\n",
    "\n",
    "        stats['y_test'] = _y_test\n",
    "        stats['y_pred'] = _y_pred\n",
    "\n",
    "        with open(dir_rootpath + \"{}_{}.p\".format(savedir, exp_id), 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "    # compute average MLAE, MSE and SD.\n",
    "        MLAE_trains = []\n",
    "        MLAE_tests = []\n",
    "\n",
    "        MSE_trains = []\n",
    "        MSE_tests = []\n",
    "\n",
    "\n",
    "        with open(dir_rootpath + \"{}_{}.p\".format(savedir, exp_id), 'rb') as f:\n",
    "            stats = pickle.load(f)\n",
    "\n",
    "            MLAE_trains.append(stats['MLAE_train'])\n",
    "            MLAE_tests.append(stats['MLAE_test'])\n",
    "            MSE_trains.append(stats['MSE_train'])\n",
    "            MSE_tests.append(stats['MSE_test'])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "        with open(dir_rootpath + \"{}_avg.p\".format(savedir), 'wb') as f:\n",
    "            stats = dict()\n",
    "\n",
    "            stats['MSE_train_avg'] = np.average(MSE_trains)\n",
    "            stats['MSE_test_avg'] = np.average(MSE_tests)\n",
    "            stats['MSE_train_SD'] = np.std(MSE_trains)\n",
    "            stats['MSE_test_SD'] = np.std(MSE_tests)\n",
    "\n",
    "            stats['MLAE_train_avg'] = np.average(MLAE_trains)\n",
    "            stats['MLAE_test_avg'] = np.average(MLAE_tests)\n",
    "            stats['MLAE_train_SD'] = np.std(MLAE_trains)\n",
    "            stats['MLAE_test_SD'] = np.std(MLAE_tests)\n",
    "            pickle.dump(stats, f)\n",
    "\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6de5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c48ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a868e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
